{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "similar-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data  Extraction and Text Analysis - Task for Data Science Internship at Blackcoffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "seeing-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import urllib.request\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "import html\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "absolute-wisconsin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000003662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06 00:00:00</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000003662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15 00:00:00</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000003662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13 00:00:00</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000003662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12 00:00:00</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0000003662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16 00:00:00</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0         CIK            CONAME   FYRMO                FDATE     FORM  \\\n",
       "1  0000003662  SUNBEAM CORP/FL/  199803  1998-03-06 00:00:00  10-K405   \n",
       "2  0000003662  SUNBEAM CORP/FL/  199805  1998-05-15 00:00:00     10-Q   \n",
       "3  0000003662  SUNBEAM CORP/FL/  199808  1998-08-13 00:00:00  NT 10-Q   \n",
       "4  0000003662  SUNBEAM CORP/FL/  199811  1998-11-12 00:00:00   10-K/A   \n",
       "5  0000003662  SUNBEAM CORP/FL/  199811  1998-11-16 00:00:00  NT 10-Q   \n",
       "\n",
       "0                                  SECFNAME  \n",
       "1  edgar/data/3662/0000950170-98-000413.txt  \n",
       "2  edgar/data/3662/0000950170-98-001001.txt  \n",
       "3  edgar/data/3662/0000950172-98-000783.txt  \n",
       "4  edgar/data/3662/0000950170-98-002145.txt  \n",
       "5  edgar/data/3662/0000950172-98-001203.txt  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the data from the Excel file into a dataFrame and eader column names are being assigned as the dataframe column names\n",
    "input_file  = pd.read_excel(\"ciklist.xls\", header=None) \n",
    "header  = input_file.iloc[0,:]                          \n",
    "input_file.columns  = header                                      \n",
    "secfname_or  = input_file.copy()                             \n",
    "secfname_or       = secfname_or.iloc[1:,5]                      \n",
    "file  = input_file.iloc[1:,:]                         \n",
    "file.head()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "spanish-nirvana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000003662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06 00:00:00</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000003662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15 00:00:00</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000003662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13 00:00:00</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000003662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12 00:00:00</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0000003662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16 00:00:00</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0         CIK            CONAME   FYRMO                FDATE     FORM  \\\n",
       "1  0000003662  SUNBEAM CORP/FL/  199803  1998-03-06 00:00:00  10-K405   \n",
       "2  0000003662  SUNBEAM CORP/FL/  199805  1998-05-15 00:00:00     10-Q   \n",
       "3  0000003662  SUNBEAM CORP/FL/  199808  1998-08-13 00:00:00  NT 10-Q   \n",
       "4  0000003662  SUNBEAM CORP/FL/  199811  1998-11-12 00:00:00   10-K/A   \n",
       "5  0000003662  SUNBEAM CORP/FL/  199811  1998-11-16 00:00:00  NT 10-Q   \n",
       "\n",
       "0                                           SECFNAME  \n",
       "1  https://www.sec.gov/Archives/edgar/data/3662/0...  \n",
       "2  https://www.sec.gov/Archives/edgar/data/3662/0...  \n",
       "3  https://www.sec.gov/Archives/edgar/data/3662/0...  \n",
       "4  https://www.sec.gov/Archives/edgar/data/3662/0...  \n",
       "5  https://www.sec.gov/Archives/edgar/data/3662/0...  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now adding https://www.sec.gov/Archives/ to every cells of column F (ciklist.xls) to access link to the financial report as mentioned in the objective\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "#Add the below link at the prefix of all the values in SECFNAME column\n",
    "link = 'https://www.sec.gov/Archives/'\n",
    "file['SECFNAME'] = link + file['SECFNAME'].astype(str)\n",
    "\n",
    "#Now all the values in the SECFNAME column have become hyperlinks linking to \n",
    "#HTML pages\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "finite-episode",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CIK', 'CONAME', 'FYRMO', 'FDATE', 'FORM', 'SECFNAME',\n",
       "       'mda_positive_score', 'mda_negative_score', 'mda_polarity_score',\n",
       "       'mda_average_sentence_length', 'mda_percentage_of_complex_words',\n",
       "       'mda_fog_index', 'mda_complex_word_count', 'mda_word_count',\n",
       "       'mda_uncertainty_score', 'mda_constraining_score',\n",
       "       'mda_positive_word_proportion', 'mda_negative_word_proportion',\n",
       "       'mda_uncertainty_word_proportion', 'mda_constraining_word_proportion',\n",
       "       'qqdmr_positive_score', 'qqdmr_negative_score', 'qqdmr_polarity_score',\n",
       "       'qqdmr_average_sentence_length', 'qqdmr_percentage_of_complex_words',\n",
       "       'qqdmr_fog_index', 'qqdmr_complex_word_count', 'qqdmr_word_count',\n",
       "       'qqdmr_uncertainty_score', 'qqdmr_constraining_score',\n",
       "       'qqdmr_positive_word_proportion', 'qqdmr_negative_word_proportion',\n",
       "       'qqdmr_uncertainty_word_proportion',\n",
       "       'qqdmr_constraining_word_proportion', 'rf_positive_score',\n",
       "       'rf_negative_score', 'rf_polarity_score', 'rf_average_sentence_length',\n",
       "       'rf_percentage_of_complex_words', 'rf_fog_index',\n",
       "       'rf_complex_word_count', 'rf_word_count', 'rf_uncertainty_score',\n",
       "       'rf_constraining_score', 'rf_positive_word_proportion',\n",
       "       'rf_negative_word_proportion', 'rf_uncertainty_word_proportion',\n",
       "       'rf_constraining_word_proportion', 'constraining_words_whole_report'],\n",
       "      dtype='object', name=0)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_file =  pd.read_excel(\"Output Data Structure.xls\", header=None) \n",
    "Header = output_file.iloc[0,:] \n",
    "output_file.columns  = Header\n",
    "outfile  = output_file.iloc[1:,:] \n",
    "outfile.columns\n",
    "#extracting columns to merge with input file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "aware-yugoslavia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "      <th>mda_positive_score</th>\n",
       "      <th>mda_negative_score</th>\n",
       "      <th>mda_polarity_score</th>\n",
       "      <th>mda_average_sentence_length</th>\n",
       "      <th>...</th>\n",
       "      <th>rf_fog_index</th>\n",
       "      <th>rf_complex_word_count</th>\n",
       "      <th>rf_word_count</th>\n",
       "      <th>rf_uncertainty_score</th>\n",
       "      <th>rf_constraining_score</th>\n",
       "      <th>rf_positive_word_proportion</th>\n",
       "      <th>rf_negative_word_proportion</th>\n",
       "      <th>rf_uncertainty_word_proportion</th>\n",
       "      <th>rf_constraining_word_proportion</th>\n",
       "      <th>constraining_words_whole_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000003662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06 00:00:00</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000003662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15 00:00:00</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000003662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13 00:00:00</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          CIK            CONAME   FYRMO                FDATE     FORM  \\\n",
       "0  0000003662  SUNBEAM CORP/FL/  199803  1998-03-06 00:00:00  10-K405   \n",
       "1  0000003662  SUNBEAM CORP/FL/  199805  1998-05-15 00:00:00     10-Q   \n",
       "2  0000003662  SUNBEAM CORP/FL/  199808  1998-08-13 00:00:00  NT 10-Q   \n",
       "\n",
       "                                            SECFNAME mda_positive_score  \\\n",
       "0  https://www.sec.gov/Archives/edgar/data/3662/0...                NaN   \n",
       "1  https://www.sec.gov/Archives/edgar/data/3662/0...                NaN   \n",
       "2  https://www.sec.gov/Archives/edgar/data/3662/0...                NaN   \n",
       "\n",
       "  mda_negative_score mda_polarity_score mda_average_sentence_length  ...  \\\n",
       "0                NaN                NaN                         NaN  ...   \n",
       "1                NaN                NaN                         NaN  ...   \n",
       "2                NaN                NaN                         NaN  ...   \n",
       "\n",
       "  rf_fog_index rf_complex_word_count rf_word_count rf_uncertainty_score  \\\n",
       "0          NaN                   NaN           NaN                  NaN   \n",
       "1          NaN                   NaN           NaN                  NaN   \n",
       "2          NaN                   NaN           NaN                  NaN   \n",
       "\n",
       "  rf_constraining_score rf_positive_word_proportion  \\\n",
       "0                   NaN                         NaN   \n",
       "1                   NaN                         NaN   \n",
       "2                   NaN                         NaN   \n",
       "\n",
       "  rf_negative_word_proportion rf_uncertainty_word_proportion  \\\n",
       "0                         NaN                            NaN   \n",
       "1                         NaN                            NaN   \n",
       "2                         NaN                            NaN   \n",
       "\n",
       "  rf_constraining_word_proportion constraining_words_whole_report  \n",
       "0                             NaN                             NaN  \n",
       "1                             NaN                             NaN  \n",
       "2                             NaN                             NaN  \n",
       "\n",
       "[3 rows x 49 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now connecting the required output data structure new coumns to file\n",
    "new_columns = ['mda_positive_score', 'mda_negative_score', 'mda_polarity_score', 'mda_average_sentence_length', 'mda_percentage_of_complex_words', 'mda_fog_index',\n",
    "               'mda_complex_word_count', 'mda_word_count', 'mda_uncertainty_score', 'mda_constraining_score', 'mda_positive_word_proportion', 'mda_negative_word_proportion',\n",
    "               'mda_uncertainty_word_proportion', 'mda_constraining_word_proportion','qqdmr_positive_score','qqdmr_negative_score',\n",
    "               'qqdmr_polarity_score','qqdmr_average_sentence_length','qqdmr_percentage_of_complex_words', 'qqdmr_fog_index','qqdmr_complex_word_count',\n",
    "               'qqdmr_word_count', 'qqdmr_uncertainty_score', 'qqdmr_constraining_score', 'qqdmr_positive_word_proportion', 'qqdmr_negative_word_proportion',\n",
    "               'qqdmr_uncertainty_word_proportion', 'qqdmr_constraining_word_proportion', 'rf_positive_score', 'rf_negative_score',\n",
    "               'rf_polarity_score', 'rf_average_sentence_length', 'rf_percentage_of_complex_words', 'rf_fog_index', 'rf_complex_word_count',\n",
    "               'rf_word_count', 'rf_uncertainty_score', 'rf_constraining_score', 'rf_positive_word_proportion', 'rf_negative_word_proportion',\n",
    "               'rf_uncertainty_word_proportion', 'rf_constraining_word_proportion', 'constraining_words_whole_report']\n",
    "\n",
    "file = pd.concat([file,pd.DataFrame(columns=new_columns)]) \n",
    "file = file.reset_index()                                  \n",
    "del file['index']                                              \n",
    "file.head()                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "tropical-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching the Stop words list\n",
    "with open('StopWords_GenericLong.txt') as f:\n",
    "    Stopwords = list(f) \n",
    "    Stopwords = [i.rstrip() for i in Stopwords]\n",
    "    Stopwords = [j.lower() for j in Stopwords]\n",
    "    \n",
    "#Fetching the list of Positive and Negative words\n",
    "Positive_words = pd.read_excel(\"LoughranMcDonald_SentimentWordLists_2018.xls\", \"Positive\", header=None)\n",
    "Negative_words = pd.read_excel(\"LoughranMcDonald_SentimentWordLists_2018.xls\", \"Negative\", header=None)\n",
    "\n",
    "#Seperating all the positive and Negative words into two separate lists\n",
    "Positive_words = Positive_words[0]                           \n",
    "Negative_words = Negative_words[0]\n",
    "\n",
    "#Keeping all the positive and Negative words which are not present in stop words list\n",
    "Positive_cleaned = list(set(Positive_words)-set(Stopwords)) \n",
    "Negative_cleaned = list(set(Negative_words)-set(Stopwords)) \n",
    "\n",
    "#Converting the Positive and Negative words to Lowercase\n",
    "Positive_cleaned = [i.lower() for i in Positive_cleaned]\n",
    "Negative_cleaned = [i.lower() for i in Negative_cleaned]\n",
    "\n",
    "#Fetching the uncertainity and constarining words from the Excel sheet\n",
    "Uncertainity_dict = pd.read_excel(\"uncertainty_dictionary.xls\", header=None)\n",
    "Constraining_dict = pd.read_excel(\"constraining_dictionary.xls\", header=None)\n",
    "\n",
    "#Passing all the above obtained words into separate lists and converting it to Lowercase\n",
    "Uncertainity_words = Uncertainity_dict[0].tolist()\n",
    "Uncertainity_words = [i.lower() for i in Uncertainity_words]\n",
    "\n",
    "Constraining_words = Constraining_dict[0].tolist()\n",
    "Constraining_words = [i.lower() for i in Constraining_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "engaging-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for calculating the average length of the sentence\n",
    "def avg_sentence_len(paragraph):\n",
    "    #Calculating the number of sentences in the whole data\n",
    "    no_sentence = len(sent_tokenize(paragraph))                               \n",
    "    tokens = nltk.word_tokenize(paragraph)                               \n",
    "    no_words = len(tokens)                                        \n",
    "    #Calculating the Average Sentence length\n",
    "    if (no_sentence != 0):\n",
    "        avg_sentence_len = round(no_words/no_sentence)                     \n",
    "    else:\n",
    "        avg_sentence_len = 0\n",
    "    return avg_sentence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "found-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(data1, data2, data3):\n",
    "    # here we get all the dat collected from the site, then tokenize the list and conbine  all the cleaned data into string of words\n",
    "    if (data1 != 0):\n",
    "        data1 = nltk.word_tokenize(data1)                                     \n",
    "        data1 = [word for word in data1 if word.lower() not in Stopwords]  \n",
    "        data1 = ' '.join(data1)                                                                                    \n",
    "    if (data2 != 0):\n",
    "        data2 = nltk.word_tokenize(data2)                                   \n",
    "        data2 = [word for word in data2 if word.lower() not in Stopwords]  \n",
    "        data2 = ' '.join(data2)                                                                      \n",
    "    if (data3 != 0):\n",
    "        data3 = nltk.word_tokenize(data3)                                   \n",
    "        data3 = [word for word in data3 if word.lower() not in Stopwords]  \n",
    "        data3 = ' '.join(data3)                                             \n",
    "\n",
    "    return data1, data2, data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "homeless-sustainability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_sections_data(data):\n",
    "    # \"MANAGEMENT'S DISCUSSION AND ANALYSIS\" section     \n",
    "    topic1 = \"MANAGEMENT'S DISCUSSION AND ANALYSIS\"\n",
    "    topic1_start = [m.start() for m in re.finditer(topic1, data)]\n",
    "\n",
    "    if (len(topic1_start) == 0):\n",
    "        topic1 = \"MANAGEMENTS DISCUSSION AND ANALYSIS\"\n",
    "        topic1_start = [m.start() for m in re.finditer(topic1, data)]\n",
    "\n",
    "    if (len(topic1_start) != 0):\n",
    "    #topic_1_data has the data from the point where topic1 begins\n",
    "        topic_1_data = data[topic1_start[0]:]\n",
    "        item = \"ITEM\"\n",
    "        topic1_end = [m.start() for m in re.finditer(item, topic_1_data)]\n",
    "        if (len(topic1_end) == 0):\n",
    "            topic_1_data = topic_1_data[0:]\n",
    "        else:  \n",
    "            topic_1_data = topic_1_data[0:topic1_end[0]-1]\n",
    "    #topic_1_data ends at the point where topic1 ends\n",
    "    if (len(topic1_start) == 0):\n",
    "        topic_1_data = 0  \n",
    "     \n",
    "     #SECTION 2 - Quantitative and Qualitative Disclosures \n",
    "    topic2 = \"QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK\"\n",
    "    topic2_start = [m.start() for m in re.finditer(topic2, data)]\n",
    "  \n",
    "    if (len(topic2_start) != 0):\n",
    "        topic_2_data = data[topic2_start[0]:]\n",
    "        item = \"ITEM\"\n",
    "        topic2_end = [m.start() for m in re.finditer(item, topic_2_data)]\n",
    "        if (len(topic2_end) == 0):\n",
    "            topic_2_data = topic_2_data[0:]\n",
    "        else:  \n",
    "            topic_2_data = topic_2_data[0:topic2_end[0]-1]     \n",
    "  \n",
    "    if (len(topic2_start) == 0):\n",
    "        topic_2_data = 0 \n",
    "\n",
    "  #SECTION 3 - Risk Factors\n",
    "    topic3 = \"RISK FACTORS\"\n",
    "    topic3_start = [m.start() for m in re.finditer(topic3, data)]\n",
    "\n",
    "    if (len(topic3_start) != 0):\n",
    "        topic_3_data = data[topic3_start[0]:]\n",
    "        item = \"ITEM\"\n",
    "        topic3_end = [m.start() for m in re.finditer(item, topic_3_data)]\n",
    "        if (len(topic3_end) == 0):\n",
    "            topic_3_data = topic_3_data[0:]\n",
    "        else:  \n",
    "            topic_3_data = topic_3_data[0:topic3_end[0]-1]    \n",
    "\n",
    "    if (len(topic3_start) == 0):\n",
    "        topic_3_data = 0         \n",
    "\n",
    "    return topic_1_data, topic_2_data, topic_3_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "korean-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constr_whole_report(x):\n",
    "    #Removing the Punctuations from the string before tokenizing & tokenizing the string into a list of words\n",
    "    x      = x.translate(str.maketrans('','',string.punctuation)) \n",
    "    tokens = nltk.word_tokenize(x)                                \n",
    "    tokens = [x.lower() for x in tokens]                          \n",
    "\n",
    "    constr_whole_words = 0\n",
    "    for word in tokens:                                         \n",
    "\n",
    "        if word in Constraining_words:                                              \n",
    "            constr_whole_words += 1     \n",
    "      \n",
    "    return constr_whole_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "entertaining-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functrion for counting the word count and complex word count\n",
    "def complex_word_count(word, complex_count):\n",
    "    word   = word.translate(str.maketrans('','',string.punctuation)) #Removing the Punctuations\n",
    "    tokens = nltk.word_tokenize(word)                                \n",
    "    no_words = len(tokens)                                           \n",
    "    complex_count = 0                                                \n",
    "    for word in tokens:\n",
    "        word = word.lower()                                            \n",
    "        vowels = \"aeiou\"    \n",
    "        if (word.endswith((\"es\", \"ed\"))):  #We are ignoring the words ending with es or ed in our calculation\n",
    "            count = 0\n",
    "        else:\n",
    "            count = 0\n",
    "        for c in word:\n",
    "            if (c in vowels):\n",
    "                count = count + 1   \n",
    "    #Counting the number of Vowels in each word,if number of vowels > 2 then we are incrementing complex_count variable by 1\n",
    "        if (count > 2):                                             \n",
    "            complex_count = complex_count + 1 \n",
    "    percentage_complex_words = complex_count/no_words                      \n",
    "    return no_words, complex_count, percentage_complex_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "liberal-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION FOR CALCULATING POSITIVE SCORE, NEGATIVE SCORE AND POLARITY SCORE   \n",
    "def positive_negative_pololarity_score(data):\n",
    "\n",
    "    data   = data.translate(str.maketrans('','',string.punctuation)) \n",
    "    tokens = nltk.word_tokenize(data)                                #Tokenizing the string into a list of words\n",
    "    tokens = [data.lower() for data in tokens]                          \n",
    "    positive_words = negative_words = 0\n",
    "\n",
    "    for word in tokens:\n",
    "        if word in Positive_cleaned:\n",
    "            positive_words += 1\n",
    "        if word in Negative_cleaned:\n",
    "            negative_words -= 1 \n",
    "\n",
    "    positive_len = positive_words\n",
    "    negative_len = negative_words * -1\n",
    "    polarity_score = (positive_len - negative_len)/((positive_len + negative_len) + 0.000001)\n",
    "    return positive_len, negative_len, polarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "strange-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for calculating uncertainty and constraining score \n",
    "#If word is present in uncertainity word list we increment uncertainty_words\n",
    "#If word is present in constaining word list we increment constraining_words\n",
    "def uncertainty_constraining_score(data):\n",
    "    data   = data.translate(str.maketrans('','',string.punctuation)) \n",
    "    tokens = nltk.word_tokenize(data)                                \n",
    "    tokens = [data.lower() for x in tokens]                          \n",
    "    uncertainty_words = constraining_words = 0\n",
    "    for word in tokens:                                         \n",
    "        if word in Uncertainity_words:                              \n",
    "            uncertainty_words  += 1\n",
    "        if word in Constraining_words:                                             \n",
    "            constraining_words += 1 \n",
    "    uncertainty_count  = uncertainty_words\n",
    "    constraining_count = constraining_words\n",
    "    return uncertainty_count, constraining_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "small-ordinance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All operations on the dataframe have finished successfully\n"
     ]
    }
   ],
   "source": [
    "## we here apply loop through the column od file. In every iteration we fetch url data from the SECFNAME column in the file dataframe.\n",
    "for i in range(len(file)):                    \n",
    "  \n",
    "    url = file.iloc[i,5]                        \n",
    "    data = urllib.request.urlopen(url).readlines()  #Reading the data from the URL link page\n",
    "  \n",
    "   # Decoding the variable to remove bytes object at he begining and removing blank lines and new line character\n",
    "    data = [str(s, 'utf-8') for s in data]          \n",
    "    data = [s.strip('\\n') for s in data]            \n",
    "    data = list(filter(None, data))                 \n",
    "  #After doing the above operations the number of iterations required now would be less\n",
    "\n",
    "  #Converting the list of strings into a single string to increase speed of operation instead of looping again\n",
    "    data = \" \".join(data)\n",
    "\n",
    "  #Removing the Tables from the data\n",
    "    data = re.sub(\"(?is)<table[^>]*>(.*?)<\\/table>\", \"\", data)\n",
    "\n",
    "  #REGEX operations to remove the HTML content from our data and get a clean text data\n",
    "    html_regex = re.compile(r'<.*?>')\n",
    "    data = re.sub(html_regex,'',data)\n",
    "    data = data.replace('&nbsp;','')\n",
    "    input_data = re.sub(r'&#\\d+;', '', data)   \n",
    "\n",
    "    constr_whole_count = constr_whole_report(input_data)\n",
    "    file.loc[i, 'constraining_words_whole_report'] = constr_whole_count\n",
    "\n",
    "  #Fetching the sections data using the fetch_sections_data()\n",
    "    topic1_data, topic2_data, topic3_data = fetch_sections_data(input_data)\n",
    "\n",
    "  #Removing the Stop words, punctuations in our data\n",
    "    topic1_data, topic2_data, topic3_data = data_cleaning(topic1_data, topic2_data, topic3_data)\n",
    "\n",
    "\n",
    "# values for \"MANAGEMENT'S DISCUSSION AND ANALYSIS\" section   |\n",
    "    if (topic1_data != 0):\n",
    "    #Calculating the positive score, negative score and Polarity score\n",
    "        pos_score1, neg_score1, pol_score1 = positive_negative_pololarity_score(topic1_data)\n",
    "\n",
    "        file.loc[i, 'mda_positive_score'] = pos_score1\n",
    "        file.loc[i, 'mda_negative_score'] = neg_score1 \n",
    "        file.loc[i, 'mda_polarity_score'] = pol_score1\n",
    "\n",
    "    #Calculating the average sentence length for our section of data\n",
    "        avg_sent_len_val1 = avg_sentence_len(topic1_data)\n",
    "        file.loc[i, 'mda_average_sentence_length'] = avg_sent_len_val1\n",
    "\n",
    "    #Calculating the word count, complex word count and percentage of complex words\n",
    "        complex_count = 0\n",
    "        word_count1, complex_count1, perc_complex_words1 = complex_word_count(topic1_data, complex_count)\n",
    "        file.loc[i, 'mda_word_count']  = word_count1\n",
    "        file.loc[i, 'mda_complex_word_count'] = complex_count1\n",
    "        file.loc[i, 'mda_percentage_of_complex_words'] = perc_complex_words1\n",
    "\n",
    "    #Calculating the Fog index value \n",
    "        fog_index1 = 0.4 * (avg_sent_len_val1 + perc_complex_words1)\n",
    "        file.loc[i, 'mda_fog_index'] = fog_index1\n",
    "\n",
    "    #Calculating the uncertainity score and constraining score\n",
    "        uncer_score1, constr_score1 = uncertainty_constraining_score(topic1_data)\n",
    "        perc_uncer1  = uncer_score1/word_count1\n",
    "        perc_constr1 = constr_score1/word_count1\n",
    "        file.loc[i, 'mda_uncertainty_score']  = uncer_score1\n",
    "        file.loc[i, 'mda_constraining_score']  = constr_score1\n",
    "\n",
    "    #Calculating the Uncertainity and Constraining word proportion\n",
    "        file.loc[i, 'mda_uncertainty_word_proportion']  = perc_uncer1\n",
    "        file.loc[i, 'mda_constraining_word_proportion'] = perc_constr1\n",
    "    \n",
    "    #Calculating the Positive and Negative word proportion\n",
    "        perc_positive1 = pos_score1/word_count1\n",
    "        perc_negative1 = neg_score1/word_count1\n",
    "        file.loc[i, 'mda_positive_word_proportion']  = perc_positive1\n",
    "        file.loc[i, 'mda_negative_word_proportion']  = perc_negative1\n",
    "\n",
    "    else:\n",
    "    #If the section 1 data is not available, all the below values should be 0\n",
    "        file.iloc[i, 6:21] = 0\n",
    "\n",
    "#  Calculating the values for QUANTITATIVE AND QUALITATIVE DISCLOSURES ABOUT MARKET RISK   \n",
    "    if (topic2_data != 0):  \n",
    "    \n",
    "    #Calculating the positive score, negative score and Polarity score\n",
    "        pos_score2, neg_score2, pol_score2 = positive_negative_pololarity_score(topic2_data)\n",
    "    \n",
    "        file.loc[i, 'qqdmr_positive_score'] = pos_score2\n",
    "        file.loc[i, 'qqdmr_negative_score'] = neg_score2 \n",
    "        file.loc[i, 'qqdmr_polarity_score'] = pol_score2\n",
    "    #Calculating the average sentence length for our section of data\n",
    "        avg_sent_len_val2 = avg_sentence_len(topic2_data)\n",
    "        file.loc[i, 'qqdmr_average_sentence_length'] = avg_sent_len_val2\n",
    "    #Calculating the word count, complex word count and percentage of complex words\n",
    "        complex_count = 0\n",
    "        word_count2, complex_count2, perc_complex_words2 = complex_word_count(topic2_data, complex_count)\n",
    "        file.loc[i, 'qqdmr_word_count']  = word_count2\n",
    "        file.loc[i, 'qqdmr_complex_word_count']  = complex_count2\n",
    "        file.loc[i, 'qqdmr_percentage_of_complex_words'] = perc_complex_words2\n",
    "\n",
    "    #Calculating the Fog index value \n",
    "        fog_index2 = 0.4 * (avg_sent_len_val2 + perc_complex_words2)\n",
    "        file.loc[i, 'qqdmr_fog_index'] = fog_index2\n",
    "    #Calculating the uncertainity score and constraining score\n",
    "        uncer_score2, constr_score2 = uncertainty_constraining_score(topic2_data)\n",
    "        perc_uncer2  = uncer_score2/word_count2\n",
    "        perc_constr2 = constr_score2/word_count2\n",
    "        file.loc[i, 'qqdmr_uncertainty_score']  = uncer_score2\n",
    "        file.loc[i, 'qqdmr_constraining_score']  = constr_score2\n",
    "    #Calculating the Uncertainity and Constraining word proportion\n",
    "        file.loc[i, 'qqdmr_uncertainty_word_proportion']  = perc_uncer2\n",
    "        file.loc[i, 'qqdmr_constraining_word_proportion'] = perc_constr2\n",
    "    #Calculating the Positive and Negative word proportion\n",
    "        perc_positive2 = pos_score2/word_count2\n",
    "        perc_negative2 = neg_score2/word_count2\n",
    "        file.loc[i, 'qqdmr_positive_word_proportion']  = perc_positive2\n",
    "        file.loc[i, 'qqdmr_negative_word_proportion']  = perc_negative2\n",
    "    else:\n",
    "    #If the section 2 data is not available, all the below values should be 0\n",
    "        file.iloc[i, 20:33] = 0\n",
    "\n",
    "\n",
    "# Calculating the values for Risk factors section                  \n",
    "\n",
    "    if (topic3_data != 0):  \n",
    "    \n",
    "    #Calculating the positive score, negative score and Polarity score\n",
    "        pos_score3, neg_score3, pol_score3 = positive_negative_pololarity_score(topic3_data)\n",
    "    \n",
    "        file.loc[i, 'rf_positive_score'] = pos_score3\n",
    "        file.loc[i, 'rf_negative_score'] = neg_score3 \n",
    "        file.loc[i, 'rf_polarity_score'] = pol_score3\n",
    "    #Calculating the average sentence length for our section of data\n",
    "        avg_sent_len_val3 = avg_sentence_len(topic3_data)\n",
    "        file.loc[i, 'rf_average_sentence_length'] = avg_sent_len_val3\n",
    "    #Calculating the word count, complex word count and percentage of complex words\n",
    "        complex_count = 0\n",
    "        word_count3, complex_count3, perc_complex_words3 = complex_word_count(topic3_data, complex_count)\n",
    "        file.loc[i, 'rf_word_count']  = word_count3\n",
    "        file.loc[i, 'rf_complex_word_count']  = complex_count3\n",
    "        file.loc[i, 'rf_percentage_of_complex_words'] = perc_complex_words3\n",
    "    #Calculating the Fog index value \n",
    "        fog_index3 = 0.4 * (avg_sent_len_val3 + perc_complex_words3)\n",
    "        file.loc[i, 'rf_fog_index'] = fog_index3\n",
    "    #Calculating the uncertainity score and constraining score\n",
    "        uncer_score3, constr_score3 =uncertainty_constraining_score(topic3_data)\n",
    "        perc_uncer3  = uncer_score3/word_count3\n",
    "        perc_constr3 = constr_score3/word_count3\n",
    "        file.loc[i, 'rf_uncertainty_score'] = uncer_score3\n",
    "        file.loc[i, 'rf_constraining_score'] = constr_score3\n",
    "    #Calculating the Uncertainity and Constraining word proportion\n",
    "        file.loc[i, 'rf_uncertainty_word_proportion']  = perc_uncer3\n",
    "        file.loc[i, 'rf_constraining_word_proportion'] = perc_constr3\n",
    "    \n",
    "    #Calculating the Positive and Negative word proportion\n",
    "        perc_positive3 = pos_score3/word_count3\n",
    "        perc_negative3 = neg_score3/word_count3\n",
    "        file.loc[i, 'rf_positive_word_proportion']  = perc_positive3\n",
    "        file.loc[i, 'rf_negative_word_proportion']  = perc_negative3\n",
    "\n",
    "    else:\n",
    "    #If the section 1 data is not available, all the below values should be 0\n",
    "        file.iloc[i, 34:47] = 0\n",
    "\n",
    "  #Clearing all the variables\n",
    "    pos_score1 = neg_score1 = pol_score1 = avg_sent_len_val1 = word_count1 = complex_count1 = perc_complex_words1 = fog_index1 = uncer_score1 = constr_score1 = perc_uncer1 = perc_constr1 = perc_positive1 = perc_negative1 = 0\n",
    "    pos_score2 = neg_score2 = pol_score2 = avg_sent_len_val2 = word_count2 = complex_count2 = perc_complex_words2 = fog_index2 = uncer_score2 = constr_score2 = perc_uncer2 = perc_constr2 = perc_positive2 = perc_negative2 = 0\n",
    "    pos_score3 = neg_score3 = pol_score3 = avg_sent_len_val3 = word_count3 = complex_count3 = perc_complex_words3 = fog_index3 = uncer_score3 = constr_score3 = perc_uncer3 = perc_constr3 = perc_positive3 = perc_negative3 = 0\n",
    "    constr_whole_count = 0\n",
    "\n",
    "print(\"All operations on the dataframe have finished successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "adopted-vegetation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "      <th>mda_positive_score</th>\n",
       "      <th>mda_negative_score</th>\n",
       "      <th>mda_polarity_score</th>\n",
       "      <th>mda_average_sentence_length</th>\n",
       "      <th>...</th>\n",
       "      <th>rf_fog_index</th>\n",
       "      <th>rf_complex_word_count</th>\n",
       "      <th>rf_word_count</th>\n",
       "      <th>rf_uncertainty_score</th>\n",
       "      <th>rf_constraining_score</th>\n",
       "      <th>rf_positive_word_proportion</th>\n",
       "      <th>rf_negative_word_proportion</th>\n",
       "      <th>rf_uncertainty_word_proportion</th>\n",
       "      <th>rf_constraining_word_proportion</th>\n",
       "      <th>constraining_words_whole_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000003662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06 00:00:00</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
       "      <td>20</td>\n",
       "      <td>71</td>\n",
       "      <td>-0.56044</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000003662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15 00:00:00</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
       "      <td>11</td>\n",
       "      <td>54</td>\n",
       "      <td>-0.661538</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000003662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13 00:00:00</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000003662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12 00:00:00</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
       "      <td>40</td>\n",
       "      <td>131</td>\n",
       "      <td>-0.532164</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000003662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16 00:00:00</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0000012239</td>\n",
       "      <td>SPHERIX INC</td>\n",
       "      <td>200704</td>\n",
       "      <td>2007-04-02 00:00:00</td>\n",
       "      <td>10-K</td>\n",
       "      <td>edgar/data/12239/0001104659-07-024804.txt</td>\n",
       "      <td>109</td>\n",
       "      <td>124</td>\n",
       "      <td>-0.064378</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>6.597816</td>\n",
       "      <td>5887</td>\n",
       "      <td>11904</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011257</td>\n",
       "      <td>0.015541</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0000012239</td>\n",
       "      <td>SPHERIX INC</td>\n",
       "      <td>200705</td>\n",
       "      <td>2007-05-16 00:00:00</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/12239/0001104659-07-040463.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0000012239</td>\n",
       "      <td>SPHERIX INC</td>\n",
       "      <td>200705</td>\n",
       "      <td>2007-05-18 00:00:00</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>edgar/data/12239/0001104659-07-041441.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0000012239</td>\n",
       "      <td>SPHERIX INC</td>\n",
       "      <td>200705</td>\n",
       "      <td>2007-05-23 00:00:00</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>edgar/data/12239/0001104659-07-042333.txt</td>\n",
       "      <td>109</td>\n",
       "      <td>124</td>\n",
       "      <td>-0.064378</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>6.59721</td>\n",
       "      <td>5831</td>\n",
       "      <td>11827</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01133</td>\n",
       "      <td>0.015642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0000012239</td>\n",
       "      <td>SPHERIX INC</td>\n",
       "      <td>200708</td>\n",
       "      <td>2007-08-14 00:00:00</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>edgar/data/12239/0001104659-07-062470.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            CIK            CONAME   FYRMO                FDATE     FORM  \\\n",
       "0    0000003662  SUNBEAM CORP/FL/  199803  1998-03-06 00:00:00  10-K405   \n",
       "1    0000003662  SUNBEAM CORP/FL/  199805  1998-05-15 00:00:00     10-Q   \n",
       "2    0000003662  SUNBEAM CORP/FL/  199808  1998-08-13 00:00:00  NT 10-Q   \n",
       "3    0000003662  SUNBEAM CORP/FL/  199811  1998-11-12 00:00:00   10-K/A   \n",
       "4    0000003662  SUNBEAM CORP/FL/  199811  1998-11-16 00:00:00  NT 10-Q   \n",
       "..          ...               ...     ...                  ...      ...   \n",
       "147  0000012239       SPHERIX INC  200704  2007-04-02 00:00:00     10-K   \n",
       "148  0000012239       SPHERIX INC  200705  2007-05-16 00:00:00  NT 10-Q   \n",
       "149  0000012239       SPHERIX INC  200705  2007-05-18 00:00:00     10-Q   \n",
       "150  0000012239       SPHERIX INC  200705  2007-05-23 00:00:00   10-K/A   \n",
       "151  0000012239       SPHERIX INC  200708  2007-08-14 00:00:00     10-Q   \n",
       "\n",
       "                                      SECFNAME mda_positive_score  \\\n",
       "0     edgar/data/3662/0000950170-98-000413.txt                 20   \n",
       "1     edgar/data/3662/0000950170-98-001001.txt                 11   \n",
       "2     edgar/data/3662/0000950172-98-000783.txt                  0   \n",
       "3     edgar/data/3662/0000950170-98-002145.txt                 40   \n",
       "4     edgar/data/3662/0000950172-98-001203.txt                  0   \n",
       "..                                         ...                ...   \n",
       "147  edgar/data/12239/0001104659-07-024804.txt                109   \n",
       "148  edgar/data/12239/0001104659-07-040463.txt                  0   \n",
       "149  edgar/data/12239/0001104659-07-041441.txt                  0   \n",
       "150  edgar/data/12239/0001104659-07-042333.txt                109   \n",
       "151  edgar/data/12239/0001104659-07-062470.txt                  0   \n",
       "\n",
       "    mda_negative_score mda_polarity_score mda_average_sentence_length  ...  \\\n",
       "0                   71           -0.56044                          25  ...   \n",
       "1                   54          -0.661538                          28  ...   \n",
       "2                    0                  0                           0  ...   \n",
       "3                  131          -0.532164                          24  ...   \n",
       "4                    0                  0                           0  ...   \n",
       "..                 ...                ...                         ...  ...   \n",
       "147                124          -0.064378                          17  ...   \n",
       "148                  0                  0                           0  ...   \n",
       "149                  0                  0                           0  ...   \n",
       "150                124          -0.064378                          16  ...   \n",
       "151                  0                  0                           0  ...   \n",
       "\n",
       "    rf_fog_index rf_complex_word_count rf_word_count rf_uncertainty_score  \\\n",
       "0              0                     0             0                    0   \n",
       "1              0                     0             0                    0   \n",
       "2              0                     0             0                    0   \n",
       "3              0                     0             0                    0   \n",
       "4              0                     0             0                    0   \n",
       "..           ...                   ...           ...                  ...   \n",
       "147     6.597816                  5887         11904                    0   \n",
       "148            0                     0             0                    0   \n",
       "149            0                     0             0                    0   \n",
       "150      6.59721                  5831         11827                    0   \n",
       "151            0                     0             0                    0   \n",
       "\n",
       "    rf_constraining_score rf_positive_word_proportion  \\\n",
       "0                       0                           0   \n",
       "1                       0                           0   \n",
       "2                       0                           0   \n",
       "3                       0                           0   \n",
       "4                       0                           0   \n",
       "..                    ...                         ...   \n",
       "147                     0                    0.011257   \n",
       "148                     0                           0   \n",
       "149                     0                           0   \n",
       "150                     0                     0.01133   \n",
       "151                     0                           0   \n",
       "\n",
       "    rf_negative_word_proportion rf_uncertainty_word_proportion  \\\n",
       "0                             0                              0   \n",
       "1                             0                              0   \n",
       "2                             0                              0   \n",
       "3                             0                              0   \n",
       "4                             0                              0   \n",
       "..                          ...                            ...   \n",
       "147                    0.015541                            0.0   \n",
       "148                           0                              0   \n",
       "149                           0                              0   \n",
       "150                    0.015642                            0.0   \n",
       "151                           0                              0   \n",
       "\n",
       "    rf_constraining_word_proportion constraining_words_whole_report  \n",
       "0                               NaN                            1452  \n",
       "1                               NaN                            1029  \n",
       "2                               NaN                               5  \n",
       "3                               NaN                             691  \n",
       "4                               NaN                               4  \n",
       "..                              ...                             ...  \n",
       "147                             0.0                             130  \n",
       "148                             NaN                               0  \n",
       "149                             NaN                              21  \n",
       "150                             0.0                             129  \n",
       "151                             NaN                              34  \n",
       "\n",
       "[152 rows x 49 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final output\n",
    "#Removing the pre-fix http link at the beginning to keep values same as original file\n",
    "link = 'https://www.sec.gov/Archives/'\n",
    "secfname_or = secfname_or.reset_index(drop=True)\n",
    "file['SECFNAME'] = secfname_or \n",
    "\n",
    "#Final output is stored in file dataframe\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### END OF THE CODE       ############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
